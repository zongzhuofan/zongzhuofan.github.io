<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhuofan Zong (ÂÆóÂçìÂá°)</title>
  
  <meta name="author" content="Zhuofan Zong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhuofan Zong</name>
              <p>
                I am a second-year Ph.D. student from <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, The Chinese University of Hong Kong. I'm supervised by Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>. I received both my Bachelor's and Master's degrees from <a href="https://ev.buaa.edu.cn/">Beihang University</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=zQqJW3gAAAAJ&hl=en">Biao Leng</a>.
              </p>
              <p> 
                I worked as a research intern in the Base Model Department at SenseTime Research, working closely with <a href="https://songguanglu.github.io/">Guanglu Song</a> and <a href="https://liuyu.us/">Yu Liu</a>. During my internship at SenseTime, I was a core member of the founding team for frontline R&D projects, including the large vision foundation model, the multimodal interactive model, and the AIGC product <a href="https://miaohua.sensetime.com/">SenseMirage</a>.
              </p>
              <p>
              Feel free to reach out for research discussions, collaborations or just to have a chat!
              </p>
              <p style="text-align:center">
                <a href="mailto:zongzhuofan@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=vls0YhoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/TempleX98/">Github</a> &nbsp/&nbsp
                <a href="images/wechat.JPG">WeChat</a>
		<br>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zongzhuofan.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zongzhuofan.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in the area of <span class="highlight">Diffusion Models</span> and <span class="highlight">Multimodal Large Language Models</span>. My previous works also included <span class="highlight">2D and 3D Visual Perception</span>. (*: Equal Contribution ‚Ä†: Corresponding Author)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="easyref_stop()" onmouseover="easyref_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/easyref.png' style="width:100%">
              </div>
              <script type="text/javascript">
                function easyref_start() {
                  document.getElementById('easyref_image').style.opacity = "1";
                }

                function easyref_stop() {
                  document.getElementById('easyref_image').style.opacity = "0";
                }
                easyref_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://easyref-gen.github.io/">
                  <papertitle>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</papertitle>
                </a>
                <br>
                <strong>Zhuofan Zong</strong>,
                Dongzhi Jiang,
                Bingqi Ma,
                Guanglu Song,
                Hao Shao,
                Dazhong Shen,
                Yu Liu,
                Hongsheng Li‚Ä†
                <br>
          <em>arXiv</em>, 2024
                <br>
                <a href="https://easyref-gen.github.io/">project page</a>
          /
                <a href="https://arxiv.org/pdf/2412.09618">paper</a>
          /
                <a href="https://github.com/TempleX98/EasyRef">code</a>
                <p></p>
                <p>We present EasyRef, the first work that capable of modeling the consistent visual elements of various group image references with a single generalist multimodal LLM for diffusion models.</p>
              </td>
            </tr>

            <tr onmouseout="vividface_stop()" onmouseover="vividface_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <img src='images/vividface.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function vividface_start() {
                    document.getElementById('vividface_image').style.opacity = "1";
                  }
  
                  function vividface_stop() {
                    document.getElementById('vividface_image').style.opacity = "0";
                  }
                  vividface_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://hao-shao.com/projects/vividface.html">
                    <papertitle>VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping</papertitle>
                  </a>
                  <br>
                  Hao Shao,
                  Shulun Wang,
                  Yang Zhou,
                  Guanglu Song,
                  Dailan He,
                  Shuo Qin,
                  <strong>Zhuofan Zong</strong>,
                  Bingqi Ma,
                  Yu Liu‚Ä†,
                  Hongsheng Li‚Ä†
                  <br>
            <em>arXiv</em>, 2024
                  <br>
                  <a href="https://hao-shao.com/projects/vividface.html">project page</a>
            /
                  <a href="https://arxiv.org/pdf/2412.11279">paper</a>
            /
                  <a href="https://github.com/deepcs233/VividFace">code</a>
                  <p></p>
                  <p>We propose a diffusion-based framework for video face swapping, featuring hybrid training, an AIDT dataset, and 3D reconstruction for superior identity preservation and temporal consistency.</p>
                </td>
              </tr>

          <tr onmouseout="mova_stop()" onmouseover="mova_start()">
			      <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/mova.jpg' style="width:100%">
              </div>
			        <script type="text/javascript">
			          function mova_start() {
			            document.getElementById('mova_image').style.opacity = "1";
			          }

			          function mova_stop() {
			            document.getElementById('mova_image').style.opacity = "0";
			          }
			          mova_stop()
			        </script>
			      </td>
			            <td style="padding:10px;width:60%;vertical-align:middle">
			          <a href="https://github.com/TempleX98/MoVA/">
			            <papertitle>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</papertitle>
			          </a>
			          <br>
			          <strong>Zhuofan Zong*</strong>,
			          Bingqi Ma*,
                Dazhong Shen,
			          Guanglu Song,
                Hao Shao,
                Dongzhi Jiang,
			          Hongsheng Li‚Ä†,
                Yu Liu‚Ä†
			          <br>
			    <em>NeurIPS</em>, 2024
			          <br>
			          <a href="https://arxiv.org/pdf/2404.13046">paper</a>
			    /
			          <a href="https://github.com/TempleX98/MoVA">code</a>
			          <p></p>
			          <p>MoVA is a novel MLLM that can adaptively route and fuse multiple task-specific vision experts in
                  a coarse-to-fine mechanism, alleviating the bias of CLIP vision encoder. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods.</p>
			        </td>
			      </tr>

            <tr onmouseout="lidit_stop()" onmouseover="lidit_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <img src='images/lidit.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function lidit_start() {
                    document.getElementById('lidit_image').style.opacity = "1";
                  }
  
                  function lidit_stop() {
                    document.getElementById('lidit_image').style.opacity = "0";
                  }
                  lidit_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2406.11831">
                    <papertitle>Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models</papertitle>
                  </a>
                  <br>
                  Bingqi Ma*,
                  <strong>Zhuofan Zong*</strong>,
                  Guanglu Song,
                  Hongsheng Li,
                  Yu Liu‚Ä†
                  <br>
            <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2406.11831">paper</a>
            /
                  <a href="https://miaohua.sensetime.com/">model API</a>
                  <p></p>
                  <p>We propose to unleash the prompt encoding capability of large language models (LLMs) for diffusion models. LiDiT-10B surpasses state-of-the-art models including Stable Diffusion 3, DALL-E 3, and Midjourney V6. The proposed method is also one of the core technologies powering <a href="https://miaohua.sensetime.com/">SenseMirage</a>.</p>
                </td>
              </tr>

            <tr onmouseout="visualcot_stop()" onmouseover="visualcot_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <img src='images/visualcot.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function visualcot_start() {
                    document.getElementById('visualcot_image').style.opacity = "1";
                  }
  
                  function visualcot_stop() {
                    document.getElementById('visualcot_image').style.opacity = "0";
                  }
                  visualcot_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://hao-shao.com/projects/viscot.html">
                    <papertitle>Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</papertitle>
                  </a>
                  <br>
                  Hao Shao, 
                  Shengju Qian, 
                  Han Xiao,
                  Guanglu Song,
                  <strong>Zhuofan Zong</strong>,
                  Letian Wang,
                  Yu Liu‚Ä†,
                  Hongsheng Li‚Ä†
                  <br>
            <em>NeurIPS</em>, 2024, &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font>
                  <br>
                  <a href="https://hao-shao.com/projects/viscot.html">project page</a>
            /
                  <a href="https://arxiv.org/abs/2403.16999">paper</a>
            /
                  <a href="https://github.com/deepcs233/Visual-CoT">code</a>
                  <p></p>
                  <p>We propose Visual CoT, including a new pipeline/dataset/benchmark that enhances the interpretability of MLLMs by incorporating visual Chain-of-Thought reasoning, optimizing for complex visual inputs.</p>
                </td>
              </tr>

      <tr onmouseout="comat_stop()" onmouseover="comat_start()">
        <td style="padding:10px;width:40%;vertical-align:middle">
          <div class="one">
            <div class="two" id='comat_image'>
              <img src='images/comat_after.png' width="260" height="170"></div>
              <img src='images/comat_before.png' width="260" height="170">
          </div>
          <script type="text/javascript">
            function comat_start() {
              document.getElementById('comat_image').style.opacity = "1";
            }

            function comat_stop() {
              document.getElementById('comat_image').style.opacity = "0";
            }
            comat_stop()
          </script>
        </td>
              <td style="padding:10px;width:60%;vertical-align:middle">
            <a href="https://caraj7.github.io/">
              <papertitle>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</papertitle>
            </a>
            <br>
            Dongzhi Jiang, 
            Guanglu Song, 
            Xiaoshi Wu, 
            Renrui Zhang, 
            Dazhong Shen,
            <strong>Zhuofan Zong</strong>,
            Yu Liu‚Ä†,
            Hongsheng Li‚Ä†
            <br>
      <em>NeurIPS</em>, 2024
            <br>
            <a href="https://caraj7.github.io/">project page</a>
      /
            <a href="https://arxiv.org/abs/2404.03653">paper</a>
      /
            <a href="https://github.com/CaraJ7/CoMat">code</a>
            <p></p>
            <p>We propose a fine-tuning strategy to address the text-to-image misalignment issue with image-to-text concept matching. The training data only includes text prompts.</p>
          </td>
        </tr>

        <tr onmouseout="raphael_stop()" onmouseover="raphael_start()">
          <td style="padding:10px;width:40%;vertical-align:middle">
            <div class="one" style="width:100%">
              <img src='images/raphael.png' style="width:100%">
            </div>
            <script type="text/javascript">
              function raphael_start() {
                document.getElementById('raphael_image').style.opacity = "1";
              }

              function raphael_stop() {
                document.getElementById('raphael_image').style.opacity = "0";
              }
              raphael_stop()
            </script>
          </td>
                <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.18295">
                <papertitle>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</papertitle>
              </a>
              <br>
              Zeyue Xue*, 
              Guanglu Song*, 
              Qiushan Guo, 
              Boxiao Liu,
              <strong>Zhuofan Zong</strong>,
              Yu Liu‚Ä†,
              Ping Luo‚Ä†
              <br>
        <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2304.00967">paper</a>
        /
              <a href="https://miaohua.sensetime.com/">model API</a>
              <p></p>
              <p> RAPHAEL proposes space-MoE and time-MoE layers and achieves a state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. The proposed method is also one of the core technologies powering <a href="https://miaohua.sensetime.com/">SenseMirage</a>.</p>
            </td>
          </tr>

          <tr onmouseout="hop_stop()" onmouseover="hop_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/hop.jpg' style="width:100%">
              </div>
              <script type="text/javascript">
                function hop_start() {
                  document.getElementById('hop_image').style.opacity = "1";
                }

                function hop_stop() {
                  document.getElementById('hop_image').style.opacity = "0";
                }
                hop_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://github.com/Sense-X/HoP/">
                  <papertitle>Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction</papertitle>
                </a>
                <br>
                <strong>Zhuofan Zong*</strong>,
                Dongzhi Jiang*,
                Guanglu Song,
                Zeyue Xue,
                Jingyong Su,
                Hongsheng Li‚Ä†,
                Yu Liu‚Ä†
                <br>
          <em>ICCV</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2304.00967">paper</a>
          /
                <a href="https://github.com/Sense-X/HoP">code</a>
                <p></p>
                <p>We propose a new paradigm, named Historical Object Prediction (HoP) for multi-view 3D detection to leverage temporal information more effectively. HoP achieves 68.5% NDS and 62.4% mAP with ViT-L, outperforming all the counterparts on the <a href="https://www.nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Camera">nuScenes detection (camera-only) leaderboard</a>.</p>
              </td>
            </tr>

          <tr onmouseout="codetr_stop()" onmouseover="codetr_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/codetr.jpg' style="width:100%">
              </div>
              <script type="text/javascript">
                function codetr_start() {
                  document.getElementById('codetr_image').style.opacity = "1";
                }

                function codetr_stop() {
                  document.getElementById('codetr_image').style.opacity = "0";
                }
                codetr_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://github.com/Sense-X/Co-DETR/">
                  <papertitle>DETRs with Collaborative Hybrid Assignments Training</papertitle>
                </a>
                <br>
                <strong>Zhuofan Zong</strong>,
                Guanglu Song,
                Yu Liu‚Ä†
                <br>
          <em>ICCV</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2211.12860">paper</a>
          /
                <a href="https://github.com/Sense-X/Co-DETR">code</a>   
          /   
                <a href="https://mp.weixin.qq.com/s/68ZbM9wwIB_Hfm-J8MTumA">‰∏≠ÊñáËß£ËØª</a>   
                <p></p>
                <p>We present a novel collaborative hybrid assignments training scheme and achieve state-of-the-art performances on object detection and instance segmentation tasks. Specifically, Co-DETR is the first model to achieve 66.0 box AP and 57.0 mask AP on the <a href="https://codalab.lisn.upsaclay.fr/competitions/7384#results">COCO test-dev leaderboard</a>.</p>
              </td>
            </tr>

            <tr onmouseout="agvm_stop()" onmouseover="agvm_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <img src='images/agvm.jpg' style="width:100%">
                </div>
                <script type="text/javascript">
                  function agvm_start() {
                    document.getElementById('agvm_image').style.opacity = "1";
                  }
  
                  function agvm_stop() {
                    document.getElementById('agvm_image').style.opacity = "0";
                  }
                  agvm_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://github.com/Sense-X/AGVM/">
                    <papertitle>Large-batch Optimization for Dense Visual Predictions: Training Faster R-CNN in 4.2 Minutes</papertitle>
                  </a>
                  <br>
                  Zeyue Xue, 
                  Jianming Liang, 
                  Guanglu Song,
                  <strong>Zhuofan Zong</strong>,
                  Liang Chen,
                  Yu Liu‚Ä†,
                  Ping Luo‚Ä†
                  <br>
            <em>NeurIPS</em>, 2022
                  <br>
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/76bea0a1cf7bf9b78f842009f6de15a1-Paper-Conference.pdf">paper</a>
            /
                  <a href="https://github.com/Sense-X/AGVM">code</a>
            /
                  <a href="https://mp.weixin.qq.com/s/cmOXvXAelQoXm6ZH-9Vf2w">‰∏≠ÊñáËß£ËØª</a>
                  <p></p>
                  <p>We propose Adaptive Gradient Variance Modulator (AGVM) to train dense visual predictors with very large batch size. It enables training an object detector with one billion parameters in just 3.5 hours, reducing the training time by 20.9√ó, whilst achieving 62.2 mAP on the COCO val leaderboard.</p>
                </td>
              </tr>
                         
              <tr onmouseout="sit_stop()" onmouseover="sit_start()">
                <td style="padding:10px;width:40%;vertical-align:middle">
                  <div class="one" style="width:100%">
                    <img src='images/sit.jpg' style="width:100%">
                  </div>
                  <script type="text/javascript">
                    function sit_start() {
                      document.getElementById('sit_image').style.opacity = "1";
                    }
    
                    function sit_stop() {
                      document.getElementById('sit_image').style.opacity = "0";
                    }
                    sit_stop()
                  </script>
                </td>
                      <td style="padding:10px;width:60%;vertical-align:middle">
                    <a href="https://github.com/Sense-X/SiT/">
                      <papertitle>Self-slimmed Vision Transformer</papertitle>
                    </a>
                    <br>
                    <strong>Zhuofan Zong*</strong>,
                    Kunchang Li*,
                    Guanglu Song,
                    Yali Wang,
                    Yu Qiao,
                    Biao Leng,
                    Yu Liu‚Ä†
                    <br>
              <em>ECCV</em>, 2022
                    <br>
                    <a href="https://arxiv.org/pdf/2111.12624">paper</a>
              /
                    <a href="https://github.com/Sense-X/SiT">code</a>
                    <p></p>
                    <p>We propose a generic self-slimmed learning approach for ViT token pruning. Our method can speed up ViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x while maintaining 97% of their performance on the ImageNet-1K dataset.</p>
                  </td>
                </tr>

          <tr onmouseout="rcnet_stop()" onmouseover="rcnet_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/rcnet.jpg' style="width:100%">
              </div>
              <script type="text/javascript">
                function rcnet_start() {
                  document.getElementById('rcnet_image').style.opacity = "1";
                }

                function rcnet_stop() {
                  document.getElementById('rcnet_image').style.opacity = "0";
                }
                rcnet_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://github.com/TempleX98/RCNet/">
                  <papertitle>RCNet: Reverse Feature Pyramid and Cross-scale Shift Network for Object Detection</papertitle>
                </a>
                <br>
                <strong>Zhuofan Zong</strong>,
                Qianggang Cao,
                Biao Leng‚Ä†
                <br>
          <em>ACM MM</em>, 2021
                <br>
                <a href="https://arxiv.org/pdf/2110.12130">paper</a>
          /
                <a href="https://github.com/TempleX98/RCNet/">code</a>
                <p></p>
                <p>We introduce RCNet, a novel architecture for multiscale feature fusion in object detection, addressing the inefficiencies and limitations of traditional Feature Pyramid Networks (FPN). Experiments on the MS COCO dataset show RCNet can bring significant performance gains.</p>
              </td>
            </tr>

          <tr onmouseout="agcn_stop()" onmouseover="agcn_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <img src='images/agcn.jpg' style="width:100%">
              </div>
              <script type="text/javascript">
                function agcn_start() {
                  document.getElementById('agcn_image').style.opacity = "1";
                }

                function agcn_stop() {
                  document.getElementById('agcn_image').style.opacity = "0";
                }
                agcn_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5893">
                  <papertitle>Graph Attention Based Proposal 3D ConvNets for Action Detection</papertitle>
                </a>
                <br>
                Jun Li,
                Xianglong Liu‚Ä†,
                <strong>Zhuofan Zong</strong>,
                Wanru Zhao,
                Mingyuan Zhang,
                Jingkuan Song
                <br>
          <em>AAAI</em>, 2020
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5893">paper</a>
                <p></p>
                <p>We propose graph attention-based 3D CNNs (AGCN) for video action detection, addressing the limitations of existing models that overlook intra and inter-proposal relationships. AGCN achieves the state-of-the-art performance and improves the average mAP by 3.7% on the THUMOS 2014 dataset.</p>
              </td>
            </tr>                  

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cuhk.png">
            </td>
            <td width="75%" valign="center">
              Ph.D. student in Multimedia Lab (MMLab) @ The Chinese University of Hong Kong
              <br>
              Sep. 2023 - Now
              <br>
              Advisor: Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
            </td>
          </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/buaa_small.png">
            </td>
            <td width="75%" valign="center">
              Master in Computer Technology @ Beihang University
              <br>
              Sep. 2020 - Jan. 2023
              <br>
              Advisor: Prof. <a href="https://scholar.google.com/citations?user=zQqJW3gAAAAJ">Biao Leng</a>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/buaa_small.png"></td>
            <td width="75%" valign="center">
              Bachelor in Computer Science and Engineering @ Beihang University
              <br>
              Sep. 2016 - Jun. 2020
              <br>
              Advisor: Prof. <a href="https://scholar.google.com/citations?user=zQqJW3gAAAAJ">Biao Leng</a>
            </td>
          </tr>
       
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody></table>
      <table width="100%" align="center" border="0" cellpadding="20"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png">
            </td>
            <td width="75%" valign="center">
              SenseTime
              <br>
              Research Intern
              <br>
              Large vision fundation models
              <br>
              May, 2021 - Jan, 2025
              <br>
              Mentor: <a href="https://songguanglu.github.io/">Guanglu Song</a>
	      <br>
	      Base Model Department
            </td>
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Academic Services</heading>
              </td>
            </tr>
          </table>
          <ul>
            <li>
              <p>Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AISTATS</p>
            </li>
            <li>
               <p>Journal Reviewer: TMLR, TCSVT, PR, IMAVIS</p>
            </li>
          </ul>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Teaching</heading>
              </td>
            </tr>
          </table>
          <ul>
            <li>
              <p>Fundamentals of Applied Electromagnetics (ELEG3213), Fall 2024</p>
            </li>
            <li>
               <p>Introduction to Digital Signal Processing (ELEG3503), Spring 2025</p>
            </li>
          </ul>          

        <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7dGBaDMtZFkEIaDRUkUST06a0FVM47GKoyOmZnkq49I&cl=ffffff&w=a"></script> -->

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody></table>        
      </td>
    </tr>
  </table>
</body>

</html>
